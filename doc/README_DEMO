Howto run Trixx Demo
2021-06-09 Peter Ramm
=====================

Preconditions:
- local Oracle database with user "panorama_test" to observe
- real physical client IP-address used for Kafka, not localhost

Set your local host IP address
------------------------------
> export IP_ADDRESS=192.168.178.24

Run Kafka:
----------
> docker run --rm -p 2181:2181 -p 9092:9092 -e KAFKA_ADVERTISED_LISTENERS=LISTENER_EXT://$IP_ADDRESS:9092,LISTENER_INT://localhost:9093 \
  git.osp-dd.de:5005/main/trixx/kafka-compact

Create topic
with local KAFKA_HOME or in the just started Kafka Docker container ("docker exec -ti <container-ID> bash"):
------------------------------------------------------------------------------------------------------------
> $KAFKA_HOME/bin/kafka-topics.sh --create --topic hugo --partitions 4 --zookeeper localhost:2181 --replication-factor 1

Create Consumer for topic:
with local KAFKA_HOME or in the just started Kafka Docker container ("docker exec -ti <container-ID> bash"):
------------------------------------------------------------------------------------------------------------
$KAFKA_HOME/bin/kafka-console-consumer.sh --topic=hugo --bootstrap-server=$IP_ADDRESS:9092 --isolation-level=read_committed



Create TriTrixx config as trixx_run.yml:
----------------------------------------
> cat <<EOF >trixx_run.yml
################################
# Log level for application (debug, info, warn, error)
LOG_LEVEL: debug
# Type of used database (SQLITE, ORACLE)
TRIXX_DB_TYPE: ORACLE
# Username of TriXX schema in database
TRIXX_DB_USER: trixx_demo
# Password of TRIXX_DB_USER, also used as password of user 'admin' for GUI logon.
TRIXX_DB_PASSWORD: trixx_demo
# Database-URL for JDBC Connect: Example for Oracle: "MY_TNS_ALIAS" or "machine:port/service"
TRIXX_DB_URL: $IP_ADDRESS:1521/ORCLPDB1
# Comma separated list of seed brokers for Kafka logon, "/dev/null" for mocking Kafka connection
TRIXX_KAFKA_SEED_BROKER: $IP_ADDRESS:9092
# Initial number of worker threads
TRIXX_INITIAL_WORKER_THREADS: 3
# Max. size of Kafka message buffer per thread
TRIXX_KAFKA_TOTAL_BUFFER_SIZE_MB: 10
# Number of messages to process within one transaction
TRIXX_MAX_TRANSACTION_SIZE: 10000
# Number of messages to process within one bulk operation to kafka
TRIXX_KAFKA_MAX_BULK_COUNT: 1000
################################
EOF

Create DB-User for Trixx:
-------------------------
> docker run --rm \
  -e TRIXX_RUN_CONFIG=/etc/trixx_run.yml \
  -e TRIXX_DB_SYS_PASSWORD=oracle \
  -v $PWD/trixx_run.yml:/etc/trixx_run.yml \
  git.osp-dd.de:5005/main/trixx:master bundle exec rake ci_preparation:create_user

Run TriXX Docker container:
---------------------------
> docker run --rm \
  -e TRIXX_RUN_CONFIG=/etc/trixx_run.yml \
  -v $PWD/trixx_run.yml:/etc/trixx_run.yml \
  -p8080:8080 \
  git.osp-dd.de:5005/main/trixx:master

Create a table to observe:
--------------------------
> echo "
    CREATE TABLE Panorama_Test.Hugo (
        ID          NUMBER PRIMARY KEY,
        Name        VARCHAR2(30),
        Start_Date  DATE);
    CREATE SEQUENCE Hugo_Seq;
    GRANT SELECT ON Hugo TO Public;
  " | sqlplus panorama_test/panorama_test@$IP_ADDRESS:1521/ORCLPDB1



Configure Trixx by GUI to catch changes on table panorama_test.Hugo
-------------------------------------------------------------------
http://localhost:8080
- first login with user "admin" and passwort of DB-user
- create your own personal user, choose DB-user for authentication
- authenticate user for schema
- Connect with this user
- Configure events for table panorama_test.Hugo
- generate triggers

Generate permanent changes on panorama_test.Hugo:
-------------------------------------------------
> echo "
    BEGIN
      LOOP
        INSERT INTO Hugo (ID, Name, Start_Date) VALUES (Hugo_Seq.NextVal, 'Name '||Hugo_Seq.Currval, SYSDATE);
        COMMIT;
        DBMS_LOCK.SLEEP(1);
      END LOOP;
    END;
    /
  " | sqlplus panorama_test/panorama_test@localhost:1521/ORCLPDB1


Start ksqlDB Docker containers:
------------------
create docker-compose.yml:
----------------------------
> cat <<EOF >docker-compose.yml
---
version: '2'

services:
  ksqldb-server:
    image: confluentinc/ksqldb-server:0.11.0
    hostname: ksqldb-server
    container_name: ksqldb-server
    ports:
      - "8088:8088"
    environment:
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: $IP_ADDRESS:9092
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"

  ksqldb-cli:
    image: confluentinc/ksqldb-cli:0.11.0
    container_name: ksqldb-cli
    depends_on:
      - ksqldb-server
    entrypoint: /bin/sh
    tty: true
EOF

Start kSqlDB:
-------------
> docker-compose up

Connect to ksqkDB CLI:
-------------------
> docker exec -it ksqldb-cli ksql http://ksqldb-server:8088

Create stream from topic:
-------------------------
> CREATE STREAM hugo_stream (msg_key VARCHAR KEY,
                             id INTEGER,
                             schema VARCHAR,
                             tablename VARCHAR,
                             operation VARCHAR,
                             timestamp VARCHAR,
                             new STRUCT<NAME VARCHAR, ID INTEGER, START_DATE VARCHAR>)
  WITH (kafka_topic='hugo', value_format='JSON');

Select from stream:
-------------------
> SELECT id, schema, tablename, operation, timestamp, new->NAME, new->ID, new->Start_Date FROM hugo_stream EMIT CHANGES;
