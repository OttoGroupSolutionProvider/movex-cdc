= image:osp.png[float="left" width=200 ] MOVEX Change Data Capture: Quick start demo =
Author: Peter Ramm ( Peter.Ramm@ottogroup.com )
:Author Initials: PR
:toc: preamble
:toclevels: 4
:icons:
:imagesdir: ./images
:numbered:
:sectnumlevels: 6
:homepage: https://www.osp.de
:title-logo-image: osp.png

Implement change data tracking on an existing Oracle DB within 10 minutes

== Preconditions ==
- A running local Oracle database with an existing user to observe

TIP: Use real physical IP-addresses for Kafka configuration, not 'localhost'. Because Kafka uses this addresses also from inside the container where 'localhost' has a different content.

== Prepare Environment ==
Please adjust the values according to your setup.
----
# Physical IP address of the Docker host running Kafka and MOVEX Change Data Capture
export IP_ADDRESS=192.168.178.34

# URL of existing ORACLE DB
export DB_URL=$IP_ADDRESS:1521/ORCLPDB1

# Schema owner and password for MOVEX' internal data structures
export MOVEX_DB_USER=movex
export MOVEX_DB_PASSWORD=movex

# SYS password of Oracle instance if MOVEX schema owner should be crated by MOVEX itself
export DB_SYS_PASSWORD=oracle

# User and password of the schema to observe
export SOURCE_DB_USER=my_observed_schema
export SOURCE_DB_PASSWORD=my_observed_schema_pw
----



== Prepare Kafka ==

=== Run Kafka container ===
We use an own Docker image that combines Zookeeper and Kafka in one image
----
docker run --rm \
  -p 2181:2181 \
  -p 9092:9092 \
  -e KAFKA_ADVERTISED_LISTENERS=LISTENER_EXT://$IP_ADDRESS:9092,LISTENER_INT://localhost:9093 \
  registry.gitlab.com/otto-group-solution-provider/movex-cdc/kafka-compact:3.0.0
----

=== Create the Kafka topic ===
Run the command within in the just started Kafka Docker container ("docker exec -ti <container-ID> bash"):
----
$KAFKA_HOME/bin/kafka-topics.sh --create --topic hugo --partitions 4 --bootstrap-server $IP_ADDRESS:9092 --replication-factor 1
----

=== Create a consumer process for the topic ===
Run the command within in the just started Kafka Docker container ("docker exec -ti <container-ID> bash"):
----
$KAFKA_HOME/bin/kafka-console-consumer.sh --topic=hugo --bootstrap-server=$IP_ADDRESS:9092 --isolation-level=read_committed
----

== Prepare the MOVEX Change Data Capture application ==

=== Create config file run_config.yml ===
----
cat <<EOF >run_config.yml
################################

# Log level for application (debug, info, warn, error)
LOG_LEVEL: debug

# Type of used database (SQLITE, ORACLE)
DB_TYPE: ORACLE

# Username of TriXX schema in database
DB_USER: $MOVEX_DB_USER

# Password of DB_USER, also used as password of user 'admin' for GUI logon.
DB_PASSWORD: $MOVEX_DB_PASSWORD

# Database-URL for JDBC Connect: Example for Oracle: "MY_TNS_ALIAS" or "machine:port/service"
DB_URL: $DB_URL

# Comma separated list of seed brokers for Kafka logon, "/dev/null" for mocking Kafka connection
KAFKA_SEED_BROKER: $IP_ADDRESS:9092

################################
EOF
----

=== Create DB-User for MOVEX CDC ===
----
docker run --rm \
  -e RUN_CONFIG=/etc/run_config.yml \
  -e DB_SYS_PASSWORD=$DB_SYS_PASSWORD \
  -v $PWD/run_config.yml:/etc/run_config.yml \
  registry.gitlab.com/otto-group-solution-provider/movex-cdc:master bundle exec rake ci_preparation:create_user
----

=== Run MOVEX CDC Docker container ===
----
docker run --rm \
  -e RUN_CONFIG=/etc/run_config.yml \
  -v $PWD/run_config.yml:/etc/run_config.yml \
  -p8080:8080 \
  registry.gitlab.com/otto-group-solution-provider/movex-cdc:master
----

== Prepare test case ==

=== Create a table to observe for test user ===
----
echo "
-- Remove possibly existing objects
BEGIN
  FOR Rec IN (SELECT 1 FROM User_Tables WHERE Table_Name = 'HUGO') LOOP
    EXECUTE IMMEDIATE 'DROP TABLE HUGO';
  END LOOP;
  FOR Rec IN (SELECT 1 FROM User_Sequences WHERE Sequence_Name = 'HUGO_SEQ') LOOP
    EXECUTE IMMEDIATE 'DROP SEQUENCE HUGO_SEQ';
  END LOOP;
END;
/

CREATE TABLE Hugo (
       ID          NUMBER PRIMARY KEY,
       Name        VARCHAR2(30),
       Start_Date  DATE);
CREATE SEQUENCE Hugo_Seq;
GRANT SELECT ON Hugo TO $MOVEX_DB_USER;
GRANT FLASHBACK ON Hugo TO $MOVEX_DB_USER;
" | sqlplus $SOURCE_DB_USER/$SOURCE_DB_PASSWORD@$DB_URL
----

=== Generate permanent changes on the table to observe ===
----
echo "
  BEGIN
    LOOP
      INSERT INTO Hugo (ID, Name, Start_Date) VALUES (Hugo_Seq.NextVal, 'Name '||Hugo_Seq.Currval, SYSDATE);
      COMMIT;
      DBMS_SESSION.SLEEP(1);
    END LOOP;
  END;
/
" | sqlplus $SOURCE_DB_USER/$SOURCE_DB_PASSWORD@$DB_URL
----


== Start capture in MOVEX CDC's GUI ==
Open the application in browser: `http://localhost:8080` and login with the predefined user "admin" and the passwort of the MOVEX DB user.

image:login_admin.png[format=png, width=500]

Create your own personal application user: click "Create User"

image:users_initial.png[format=png, width=800]

In the "Create User" dialog:

* Add name and email,
* choose an existing DB-user for authentication with it's password
* check "Admin User" to allow this user administrative tasks
* Add authorized schemas where this user is enabled to configure change tracking
  ** Select a schema from the list of schemas
  ** Check "Deployment granted" to allow creation of triggers for this user
  ** Click "Add" to add this schema to the list of enabled schemas
* Click "Create "

image:create_user.png[format=png, width=800]

Logout as 'admin'

image:logout.png[format=png, width=800]

Reconnect with the just created personal user using email and the password of the associated Oracle user. +
Than choose the menu "Configuration", select the schema to observe and click "Add table to observe".

image:config_select_schema.png[format=png, width=700]

Select a table from the list and set the Kafka topic to use, than:

 * Decide wether to include the Oracle transaction ID into the event or not
 * Choose the kind of message key handling
 * Choose wether to transfer the current content of the table into the Kafka topic before tracking further changes or not
 ** Optionally place a filter condition to the initialization

image:add_table.png[format=png, width=800]

Now tap on the table to mark it as current, than configuration of columns appear.
Check the columns you want transfer to Kafka for the particular operation.

image:config_columns.png[format=png, width=800]

If you want to add filter conditions to the three operations,
than click at the filter icon for the operation and add the filter condition. +
Be aware that these conditions are executed within a trigger, so refer to columns of the table by qualifier ":new.column" or ":old.column".

image:add_filter.png[format=png, width=800]

Now all of configuration is done and the tracking can be activated. +
Head over to menu "Deployment", select one or all schemas and click "Generate for schema". +
At first only a dry run of trigger creation is executed.
All tables with difference between configured and really active triggers are shown. +
By clicking the most right triangle you may list the new trigger syntax according to the configuration as well as the optional initialization code.

image:deploy_dry.png[format=png, width=800]

Check the "Deploy" switch for the tables you want to deploy and hit "Deploy"

image:deploy_trigger.png[format=png, width=800]

Now the triggers are activated. If requested the inialization tasks are starting in background. +
After no more than one minute the MOVEX Change Data Capture will stop sleeping idle and recognize the existence of events to transfer to Kafka. +
You can evaluate the incoming events in Kafka at your already waiting consumer session.

That's it, enjoy the success (hopefully).

== Add ksqlDB  ==
This is an additional task that's not really necessary for this showcase itself. +
If you want it shows how to handle the event stream using SQL-like syntax.

=== create docker-compose.yml ===
----
cat <<EOF >docker-compose.yml
---
version: '2'

services:
  ksqldb-server:
    image: confluentinc/ksqldb-server:0.11.0
    hostname: ksqldb-server
    container_name: ksqldb-server
    ports:
      - "8088:8088"
    environment:
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: $IP_ADDRESS:9092
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"

  ksqldb-cli:
    image: confluentinc/ksqldb-cli:0.11.0
    container_name: ksqldb-cli
    depends_on:
      - ksqldb-server
    entrypoint: /bin/sh
    tty: true
EOF
----

=== Start ksqlDB ===
----
docker-compose up
----

=== Connect to ksqlDB CLI ===
----
docker exec -it ksqldb-cli ksql http://ksqldb-server:8088
----

=== Create stream from topic in ksqlDB CLI ===
----
CREATE STREAM hugo_stream (msg_key VARCHAR KEY,
                           id INTEGER,
                           schema VARCHAR,
                           tablename VARCHAR,
                           operation VARCHAR,
                           timestamp VARCHAR,
                           new STRUCT<NAME VARCHAR, ID INTEGER, START_DATE VARCHAR>)
  WITH (kafka_topic='hugo', value_format='JSON');
----

=== Select from stream in ksqlDB CLI ===
----
SELECT id, schema, tablename, operation, timestamp, new->NAME,
  new->ID, new->Start_Date FROM hugo_stream EMIT CHANGES;
----


