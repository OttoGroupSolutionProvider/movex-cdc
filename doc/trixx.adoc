= image:osp.png[float="left" width=200 ] TriXX: Change Data Capture based on DB-Triggers  =
Author: Peter Ramm ( Peter.Ramm@ottogroup.com )
:Author Initials: PR
:toc:
:toclevels: 4
:icons:
:imagesdir: ./images
:numbered:
:sectnumlevels: 6
:homepage: https://www.osp.de
:title-logo-image: osp.png

TIP: This page is not yet completed and still under construction.

== Introduction ==
=== Purpose ===
**********************************************************************
TriXX captures data change events (Insert/Update/Delete) in relational databases and immediately transfers the data changes to Kafka event hub. +
Database triggers are used to capture change events. +
The observed tables and columns are defined via TriXX' own HTML-GUI.
**********************************************************************

The events to be processed are configured via a GUI or import of configuration data. The necessary triggers are generated by the system based on the configuration data.
Synchronous processing and storage of the trigger events is initially performed locally in the Oracle database, without further dependencies on external systems.
The further transmission of the events to Kafka is asynchronous to the trigger processing.

TriXX allows a system-wide identical and redundancy-free catching of change events on database tables.
The Kafka-Cluster (Publish & Subscribe), which is supplied with events by TriXX, is responsible for the provision and distribution of events to any consumers.

The focus is on resource-conserving yet stable and high-performance processing,
low complexity in the operation of the solution and minimal intervention in the operation of the database.
In particular, compared with alternative solutions such as Oracle Golden Gate, Quest Shareplex or Red Hat Debezium,
it is not necessary to drastically increase the retention period of the DB online transaction log.

=== Concept ===

==== Using Kafka keys to ensure sequential order of messages ====
For Kafka consumers the original sequence of messages is guaranteed only for messages consumed from the same partition of a topic. +
Therefore you must place messages within the same partition of a topic if you want to consume them in original order. +
Kafka has the concept of message keys for that. Kafka ensures that messages with the same key value are placed in the same partition and this way are consumed in original sequence.

TriXX supports three kinds of message keys for Kafka that can be defined by GUI at table level:

* No message key: Messages are placed randomly in partitions
* Fixed value: All change events of a table are placed in the same partition (makes sense only if events of multiple tables are produced to the same Kafka topic)
* Primary key values: Ensures that the change history of a single DB record is always consumed in original sequence


=== Differentiation from other solutions for CDC ===
There are a number of existing solutions for change capture, commercial as well as open source.
Most of them are based on processing of DB's transaction log. +
Using transaction log for CDC ensures that no additional effort is loaded on the primary transactions,
so processing the change events is completely asynchroneous. +
But this solutions also mean:

* Covering outages of CDC target (Kafka) requires later processing of transaction log when CDC target systems become available again
* Therefore you have to preserve the transaction log in space for the longest expected outage of the CDC target, if you expect to continue processing automatically after CDC target system outage
* Including weekend, public holidays and some time for troubleshooting this regularly requires to preserve the DB transaction log in place for at least three days
* If you only need a small amount of change events from large transaction processing systems then the effort in dealing with transaction logs becomes complex and expensive compared to what you actually want.

This is the case where TriXX comes into play. +
Accepting the synchroneous overhead of triggers in business transactions the solution is sized for the expected amount of observed change events independent from the total transaction throughput of the entire database.

=== Supported databases ===

==== Oracle Database ====
Oracle Database is supported for release 12.1. and higher. +
Release 11.2 may function depending on the patch level. There are several issues with compound triggers in Oracle 11 up to release 11.2.0.4.

NOTE: Enterprise Edition with Partitioning Option in Release >= 11.2 is needed until now +
Solution for Standard Edition without partitioning is in progress

==== SQLite ====
SQLite is used as development database for TriXX. There might be no useful production use case but it works.

==== Microsoft SQL Server ====
Support for MS SQL Server is planned in the future.

==== PostgreSQL ====
Support for PostgreSQL is planned in the future.

== Operation ==
=== Preconditions for usage ===
==== Sizing of TriXX server instance ====
TODO: define mimimum CPU and memory requirements for Docker host.

==== Database schema for TriXX tables ====
TriXX needs it's own database schema at the observed database. +
This schema contains configuration tables which TriXX will create itself at first startup as well as the buffered (not yet transferred) events. +
Storage quotas for this schema should allow storage of buffered events as long as the longest possibly expected outage of Kafka that should be covered without restrictions to the business transactions.

==== Rights for owner of TriXX DB schema ====
The owner of the TriXX.schema requires the following grants at database:

.Grants required for Oracle
[cols="~,~"]
|===
|Grant|Description

|CREATE ANY TRIGGER|Allows creation and dropping of triggers in foreign schemas of database
|SELECT ON sys.DBA_Tables|Allows listing of table names for tables without SELECT grant (not included in All_Tables). This right must be granted by user SYS or SYSDBA.
|SELECT ON sys.DBA_Tab_Columns|Allows listing of column names for tables without SELECT grant (not included in All_Tab_Columns). This right must be granted by user SYS or SYSDBA.

|===

==== Kafka configuration ====
.Options for Kafka consumer
[cols="~,~,~"]
|===
|Option|Value|Description

|isolation-level|read_comitted|If not set to read_comitted the consumer will early read/consume messages of pending transactions that are possibly rolled back later from TriXX. Later successful processing of messages by TriXX may lead to duplicate occurrence of messages in consumer's stream.
|===

=== Configuring TriXX ===
You can configure TriXX either by defining config settings as environment variables or by storing configuration settings in a YML file and providing the location of this config file to TriXX via TRIXX_RUN_CONFIG.

Environment variables overrides values from configuration file.

.Mandatory environment parameters for evaluation at appliction start
[cols="~,~"]
|===
|Variable|Description

|TRIXX_DB_PASSWORD|Password ofÂ TRIXX_DB_USER, aims also as password of user 'admin' for GUI-logon. Therefore also required for database without access control like SQLite.
|TRIXX_DB_TYPE|Defines the typ of observed database. Valid values: SQLITE, ORACLE
|TRIXX_DB_URL|Database-URL for JDBC Connect:
Example for Oracle: "MY_TNS_ALIAS" or "machine:port/service"
|TRIXX_DB_USER|Username of TriXX-Schema in observed database
|TRIXX_KAFKA_SEED_BROKER|Comma-separated list of seed-brokers for Kafka logon (Host:Port), Example: "kafka1.osp-dd.de:9092, kafka2.osp-dd.de:9092"
"/dev/null" for mocking of Kafka connection in tests (discard events instead of transfer to Kafka).
|===

.Optional environment parameters for evaluation at appliction start
[cols="~,~,~"]
|===
|Variable|Description|Default value

|LOG_LEVEL|Log level of application (debug, info, warn, error)|warn
|TIMEZONE|Sets local timezone within Docker-container of TriXX-applikation. Must be directly set as environment of container, does not work from config file.|Europe/Berlin
|TNS_ADMIN|directory of tnsnames.ora for resolution of Oracle DB aliases (File tnsnames.ora is usually mounted into Docker-Container). Valid for Oracle only. Must be directly set as environment of container, does not work from config file|
|TRIXX_INITIAL_WORKER_THREADS|Initial number of worker threads. Each worker threads has it's own connection to database and Kafka and operates independent on transferring events from local DB table to Kafka.|3
|TRIXX_KAFKA_MAX_BULK_COUNT|Maximum number of messages to process within one bulk operation to Kafka. Higher values increases risk of unexpected errors like Kafka::MessageSizeTooLarge|1000
|TRIXX_KAFKA_SSL_CA_CERT|Path to CA certificate file in pem format|
|TRIXX_KAFKA_SSL_CLIENT_CERT|Path to client certifikate file in pem format|
|TRIXX_KAFKA_SSL_CLIENT_CERT_KEY|Path to client key in pem format|
|TRIXX_KAFKA_SSL_CLIENT_CERT_KEY_PASSWORD|Password for client key|
|TRIXX_KAFKA_TOTAL_BUFFER_SIZE_MB|Memory buffer size for Kafka message buffer in Megabyte. Maximum for the sum of allocated memory over all threads. If the amount is not sufficient at runtime than the value of TRIXX_KAFKA_MAX_BULK_COUNT is automatically decreased by TriXX until it is according to the available memory.|10
|TRIXX_MAX_TRANSACTION_SIZE|Maximum number of messages for processing within one transaktion (both DB and Kafka)|10000
|TRIXX_RUN_CONFIG|Path and name of configuration file in YML format as alternative to configuration by environment variables|APP_ROOT/config/trixx_run.yml
|===


==== Setup SSL-connection to Kafka ====
Kafka supports encryption and authentication via SSL.
The required setup is described at http://kafka.apache.org/documentation.html#security_ssl.
However, the certificates generated for Kafka are in JKS format, which the underlying library 'ruby-kafka' does not support.
Luckily, it is possible to convert the generated files into X509 format.
A guide how to do conversion is here: https://github.com/zendesk/ruby-kafka/wiki/Creating-X509-certificates-from-JKS-format.


=== Running TriXX ===
The TriXX application is provided as Docker-Image by:
[source]
docker pull git.osp-dd.de:5005/main/trixx

==== Start Docker container ====
You can run the this image like:
[source]
docker run -p 8080:8080 \
  --stop-timeout=120 \
  -e TRIXX_RUN_CONFIG=/etc/trixx_run.yml \
  -v /my_local_dir/trixx_run.yml:/etc/trixx_run.yml \
  git.osp-dd.de:5005/main/trixx

The web-GUI would be available by http://localhost:8080 in this case.
It is recommended to place an own reverse proxy nearby for SSL encryption.

==== Stop Docker container ====
To stop the Docker container you should provide a timeout (at "docker run" or with "docker stop") that allows TriXX to gracefully shutdown all worker threads before Docker terminates hard with "kill -9".

 docker stop -t 120 trixx

==== Uniqueness of TriXX instances ====
Depending on the database type you may run multiple TriXX instances at one database or not.

.Multiple instances allowed for TriXX
[cols="~,~,~"]
|===
|DB type|Multiple instances with same configuration (same TriXX schema)|Multiple instances with different configuration (different TriXX schemas, different Kafka targets)

|SQLite
|Not allowed: No synchronization between multiple instances exist
|Not allowed: No config-specific trigger names are used
|ORACLE
|Possible: Messages to transfer to Kafka are selected with FOR UPDATE.
|Possible: Trigger names contain numeric hash value of TriXX' owner schema. +
Therefore multiple triggers from several independent TriXX configurations at one table are possible.
|===


=== Configuration of capturing with TriXX-GUI ===
TODO: Describe GUI workflow

==== Managing users, schemas and rights ====

==== Configuring events to capture at column level ====

==== Generation of database triggers ====

=== Troubleshooting ===
==== Health check ====
There is a healthcheck service available at:

 http://<TriXX-URL>/health_check

Status code 200 (ok) is returned if the configured number of worker threads exists and is functional. +
The http-response contains a JSON-object with detailled informations.

==== Common error traps ====
===== Kafka =====
.possible problems accessing or using Kafka
[cols="~,~,~"]
|===
|Error|Description|Solution
|Log-Output: +
Disconnecting broker 0 +
Closing socket to kafka-itdr-dev:9092 +
Module: Kafka::UnknownError: Unknown error with code 53
|Error 53 means: TRANSACTIONAL_ID_AUTHORIZATION_FAILED +
The transactional id used by TriXX is not authorized to produce messages
|Explicite authorization of transactional id is required, optional as wildcard: +
kafka-acls --bootstrap-server localhost:9092 --command-config adminclient-configs.conf
--add --transactional-id * --allow-principal User:* --operation write
|===



===== Oracle-DB =====
* If TNS alias is used for TRIXX_DB_URL but no tnsnames.ora available at TNS_ADMIN then the JDBC driver treats the TNS alias as host:port:sid with several possible error messages (host does not exist etc.)

== Technical implementation ==
=== Module overview ===

==== Activities at application startup ====

The following things are executed at startup of application / docker container if necessary:

* The needed data structures in TriXX-schema (TRIXX_DB_USER) are created or updated
* The initial TriXX-user "admin" is created for GUI with link to the TRIXX_DB_USER
** For initial GUI logon with user "admin" the password is the DB-passwort of Trixx DB-user (TRIXX_DB_PASSWORD)
** The GUI user "admin" acts as supervisor with the authorization to administrate further user accounts

==== Generation of database triggers ====
==== Transfer of triggered events to Kafka ====
==== Houskeeping ====

==== Health check ====

=== API endpoints ===
Most of the API endpoints are useful only when called from GUI, but several of this API endpoints may also be useful for calling from outside TriXX. +
API Responses are JSON objects.

.API endpoints for additional usage from outside TriXX application
[cols="~,~,~,~,~"]
|===
|Verb|URL|Parameter|Response|Description

|GET|/health_check|no|JSON object with several application status info|ask health status (200=ok) and get some condensed status information
|GET|/health_check/log_file|no|current log file of application|Download log file of TriXX application. +
Requires valid user JWT in request header.
|POST|/login/do_logon|email, password|token|Validate user authentication, get JWT token for authentication/authorization of following requests
|POST|/server_controller/set_log_level|log_level (DEBUG, INFO, WARN, ERROR, FATAL)|no|Set log level of server instance, requires valid admin JWT in request header
|POST|/server_controller/set_worker_threads_count|worker_threads_count (0..200)|no|Set number of active worker threads, requires valid admin JWT in request header
|POST|/server_controller/terminate|no|no|Terminate the current TriXX process by sending SIGTERM, requires valid admin JWT in request header
|===

==== Using API endpoints from command line ====
You can use curl or wget to call API funktions with valid autorization by email and password. +
Example is for setting log level to DEBUG, adjust hostname, port, email and password to your needs. +
Needed tools are curl, jq, sed.

[source]
curl -X POST -H "Authorization: \
`curl -d "email=admin&password=trixx" http://localhost:8080/login/do_logon | \
jq .token | sed -e 's/^"//' -e 's/"$//'`" \
-d "log_level=ERROR" \
http://localhost:8080/server_control/set_log_level

=== Entity relationship model ===
image:trixx_er_model.svg.drawio[format=svg,opts=inline]

=== Security considerations ===
==== Lock user account after multiple failed logons ====
User account is locked after 5 subsequent failed logon tries. +
Unlocking a locked account is possible via GUI for admin users.

==== Suppress frequent access ====
* Email/password check at /login/do_logon is delayed for up to 5 seconds if subsequent logon requests occur within 5 seconds
* Subsequent calls to /health_check are rejected within the same second

==== Isolation between production schemas at database and TriXX ====
The TriXX application requires an own schema on database for TriXX. This schema must not contain any foreign structures.
All database changes made by TriXX are isolated to this schema.
The owner of the TriXX-schema requires only a minimum set of rights on foreign objects, especially no right to read the full table content.


