= image:osp.png[float="left" width=200 ] TriXX: Change Data Capture based on DB-Triggers  =
Author: Peter Ramm ( Peter.Ramm@ottogroup.com )
:Author Initials: PR
:toc:
:toclevels: 4
:icons:
:imagesdir: ./images
:numbered:
:sectnumlevels: 6
:homepage: https://www.osp.de
:title-logo-image: osp.png

TIP: This page is not yet completed and still under construction.

== Introduction ==
=== Purpose ===
**********************************************************************
TriXX captures data change events (Insert/Update/Delete) in relational databases and immediately transfers the data changes to Kafka event hub. +
Database triggers are used to capture change events. +
The observed tables and columns are defined via TriXX' own HTML-GUI.
**********************************************************************

The events to be processed are configured via a GUI or import of configuration data. The necessary triggers are generated by the system based on the configuration data.
Synchronous processing and storage of the trigger events is initially performed locally in the Oracle database, without further dependencies on external systems.
The further transmission of the events to Kafka is asynchronous to the trigger processing.

TriXX allows a system-wide identical and redundancy-free catching of change events on database tables.
The Kafka-Cluster (Publish & Subscribe), which is supplied with events by TriXX, is responsible for the provision and distribution of events to any consumers.

The focus is on resource-conserving yet stable and high-performance processing,
low complexity in the operation of the solution and minimal intervention in the operation of the database.
In particular, compared with alternative solutions such as Oracle Golden Gate, Quest Shareplex or Red Hat Debezium,
it is not necessary to drastically increase the retention period of the DB online transaction log.

=== Concept ===

==== Using Kafka keys to ensure sequential order of messages ====
For Kafka consumers the original sequence of messages is guaranteed only for messages consumed from the same partition of a topic. +
Therefore you must place messages within the same partition of a topic if you want to consume them in original order. +
Kafka has the concept of message keys for that. Kafka ensures that messages with the same key value are placed in the same partition and this way are consumed in original sequence.

TriXX supports three kinds of message keys for Kafka that can be defined by GUI at table level:

* No message key: Messages are placed randomly in partitions
* Fixed value: All change events of a table are placed in the same partition (makes sense only if events of multiple tables are produced to the same Kafka topic)
* Primary key values: Ensures that the change history of a single DB record is always consumed in original sequence


=== Differentiation from other solutions for CDC ===
There are a number of existing solutions for change capture, commercial as well as open source.
Most of them are based on processing of DB's transaction log. +
Using transaction log for CDC ensures that no additional effort is loaded on the primary transactions,
so processing the change events is completely asynchroneous. +
But this solutions also mean:

* Covering outages of CDC target (Kafka) requires later processing of transaction log when CDC target systems become available again
* Therefore you have to preserve the transaction log in space for the longest expected outage of the CDC target, if you expect to continue processing automatically after CDC target system outage
* Including weekend, public holidays and some time for troubleshooting this regularly requires to preserve the DB transaction log in place for at least three days
* If you only need a small amount of change events from large transaction processing systems then the effort in dealing with transaction logs becomes complex and expensive compared to what you actually want.

This is the case where TriXX comes into play. +
Accepting the synchroneous overhead of triggers in business transactions the solution is sized for the expected amount of observed change events independent from the total transaction throughput of the entire database.

=== Supported databases ===

==== Oracle Database ====
Oracle Database is supported for release 12.1. and higher. +
Release 11.2 may function depending on the patch level. There are several issues with compound triggers in Oracle 11 up to release 11.2.0.4.

NOTE: Enterprise Edition with Partitioning Option in Release >= 11.2 is needed until now +
Solution for Standard Edition without partitioning is in progress

==== SQLite ====
SQLite is used as development database for TriXX. There might be no useful production use case but it works.

==== Microsoft SQL Server ====
Support for MS SQL Server is planned in the future.

==== PostgreSQL ====
Support for PostgreSQL is planned in the future.

== Operation ==
=== Preconditions for usage ===
==== Sizing of TriXX server instance ====
TODO: define mimimum CPU and memory requirements for Docker host.

==== Database schema for TriXX tables ====
TriXX needs it's own database schema at the observed database. +
This schema contains configuration tables which TriXX will create itself at first startup as well as the buffered (not yet transferred) events. +
Storage quotas for this schema should allow storage of buffered events as long as the longest possibly expected outage of Kafka that should be covered without restrictions to the business transactions.

==== Rights for owner of TriXX DB schema ====
The owner of the TriXX schema requires some preconditions/grants at database.
The existence of this grants is checked at application start.

===== Rights for owner of TriXX DB schema: Oracle =====

.Grants required for Oracle
[cols="~,~"]
|===
|Grant|Description

|CREATE ANY TRIGGER|Allows creation and dropping of triggers in foreign schemas of database
|CREATE VIEW|Allows creation of views in TriXX schema
|RESOURCE|Allows creation of tables in own schema
|SELECT ON sys.DBA_Constraints|For primary key info of table.
|SELECT ON sys.DBA_Cons_Columns|For primary key info of table.
|SELECT ON sys.DBA_Role_Privs|Allows check if GUI-user has SELECT grant for a table.
|SELECT ON sys.DBA_Sys_Privs|Allows check if GUI-user has SELECT grant for a table.
|SELECT ON sys.DBA_Tables|Allows listing of table names for tables without SELECT grant (not included in All_Tables).
|SELECT ON sys.DBA_Tab_Columns|Allows listing of column names for tables without SELECT grant (not included in All_Tab_Columns).
|SELECT ON sys.DBA_Tab_Privs|Allows check if GUI-user has SELECT grant for a table.
|SELECT ON sys.gv_$Lock|Allows check for housekeeping if there are pending transactions. Accessed via synonym public.gv$Lock.
|SELECT ON sys.v_$Session|Allows DB session info in health check.

|===
If suitable an alternative for the detailed single grants may also be to grant 'SELECT ANY DICTIONARY' to the TriXX DB-user.

==== Kafka configuration ====
.Options for Kafka consumer
[cols="~,~,~"]
|===
|Option|Value|Description

|isolation-level|read_comitted|If not set to read_comitted the consumer will early read/consume messages of pending transactions that are possibly rolled back later from TriXX. Later successful processing of messages by TriXX may lead to duplicate occurrence of messages in consumer's stream.
|===

=== Configuring TriXX ===
You can configure TriXX either by defining config settings as environment variables or by storing configuration settings in a YML file and providing the location of this config file to TriXX via TRIXX_RUN_CONFIG.

Environment variables overrides values from configuration file.

.Mandatory environment parameters for evaluation at appliction start
[cols="~,~"]
|===
|Variable|Description

|TRIXX_DB_PASSWORD|Password ofÂ TRIXX_DB_USER, aims also as password of user 'admin' for GUI-logon. Therefore also required for database without access control like SQLite.
|TRIXX_DB_TYPE|Defines the typ of observed database. Valid values: SQLITE, ORACLE
|TRIXX_DB_URL|Database-URL for JDBC Connect:
Example for Oracle: "MY_TNS_ALIAS" or "machine:port/service"
|TRIXX_DB_USER|Username of TriXX-Schema in observed database
|TRIXX_KAFKA_SEED_BROKER|Comma-separated list of seed-brokers for Kafka logon (Host:Port), Example: "kafka1.osp-dd.de:9092, kafka2.osp-dd.de:9092"
"/dev/null" for mocking of Kafka connection in tests (discard events instead of transfer to Kafka).
|===

.Optional environment parameters for evaluation at appliction start
[cols="~,~,~"]
|===
|Variable|Description|Default value

|LOG_LEVEL|Log level of application (debug, info, warn, error)|info
|RAILS_MAX_THREADS|Maximum number of threads for the underlying Puma application server, should be set to greater than TRIXX_INITIAL_WORKER_THREADS + 30 if default is not sufficient|300
|TIMEZONE|Sets local timezone within Docker-container of TriXX-applikation. Must be directly set as environment of container, does not work from config file.|Europe/Berlin
|TNS_ADMIN|directory of tnsnames.ora for resolution of Oracle DB aliases (File tnsnames.ora is usually mounted into Docker-Container). Valid for Oracle only. Must be directly set as environment of container, does not work from config file|
|TRIXX_DB_QUERY_TIMEOUT|Maximum runtime in seconds of database query. Monitors selection on table Event_Logs. All other SQL executions are monitored by socket timeout with twice this value. |600
|TRIXX_INFO_CONTACT_PERSON|Name and email of contact person for display at GUI home screen|
|TRIXX_INITIAL_WORKER_THREADS|Initial number of worker threads. Each worker threads has it's own connection to database and Kafka and operates independent on transferring events from local DB table to Kafka.|3
|TRIXX_KAFKA_MAX_BULK_COUNT|Maximum number of messages to process within one bulk operation to Kafka. Higher values increases risk of unexpected errors like Kafka::MessageSizeTooLarge|1000
|TRIXX_KAFKA_SSL_CA_CERT|Path to CA certificate file in pem format|
|TRIXX_KAFKA_SSL_CLIENT_CERT|Path to client certifikate file in pem format|
|TRIXX_KAFKA_SSL_CLIENT_CERT_KEY|Path to client key in pem format|
|TRIXX_KAFKA_SSL_CLIENT_CERT_KEY_PASSWORD|Password for client key|
|TRIXX_KAFKA_TOTAL_BUFFER_SIZE_MB|Memory buffer size for Kafka message buffer in Megabyte. Maximum for the sum of allocated memory over all threads. If the amount is not sufficient at runtime than the value of TRIXX_KAFKA_MAX_BULK_COUNT is automatically decreased by TriXX until it is according to the available memory.|10
|TRIXX_MAX_TRANSACTION_SIZE|Maximum number of messages for processing within one transaktion (both DB and Kafka)|10000
|TRIXX_RUN_CONFIG|Path and name of configuration file in YML format as alternative to configuration by environment variables|APP_ROOT/config/trixx_run.yml
|===


==== Setup SSL-connection to Kafka ====
Kafka supports encryption and authentication via SSL.
The required setup of Kafka for SSL is described at http://kafka.apache.org/documentation.html#security_ssl.
However, the certificates generated for Kafka are in JKS format, which the underlying library 'ruby-kafka' of TriXX does not support.
Luckily, it is possible to convert the generated files into X509 format.
A guide how to do conversion is here: https://github.com/zendesk/ruby-kafka/wiki/Creating-X509-certificates-from-JKS-format.

===== Steps to convert the keystore content generated by Kafka into the formats needed for TriXX =====
Preconditions for the next steps are the openssl command line tools "keytool", "openssl"
and optionally the GUI-tool "Keystore Explorer" (https://keystore-explorer.org)

Location and passwords are used as environment variables.

*1. Extract the alias name used in client keystore file*

Identify the second alias name other than 'caroot' and use this alias in next steps for $ALIAS.

`keytool -list -v -keystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -storepass $SSL_KEYSTORE_PASSWORD | grep -i alias`

*2. Extract the signed client certificate*

`keytool -noprompt -keystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -exportcert -alias $ALIAS -rfc -storepass $SSL_KEYSTORE_PASSWORD -file client_cert.pem`

*3. a. Extract the client key with command line tools*

New client certificate key password becomes the same like source keystore password in this example.

`keytool -noprompt -srckeystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -importkeystore -srcalias $ALIAS -destkeystore cert_and_key.p12 -deststoretype PKCS12 -srcstorepass $SSL_KEYSTORE_PASSWORD -storepass $SSL_KEYSTORE_PASSWORD`

`openssl pkcs12 -in cert_and_key.p12 -nocerts -nodes -passin pass:$SSL_KEYSTORE_PASSWORD -out client_cert_key.pem`

After generation open the file 'client_cert_key.pem' in an editor and remove all attributes at top so the file content now starts with "-----BEGIN PRIVATE KEY-----".

*3. b. Extract the client key with Keystore explorer as alternative to 3. a.*

* Open file $KAFKA_CERT_DIR/kafka.client.keystore.jks im Keystore Explorer
* Choose the alias identified in step 1
* Choose menu 'Export' / 'Export private key', use format 'openssl'

*4. Extract CA certificate*

`keytool -noprompt -keystore $KAFKA_CERT_DIR/kafka.client.keystore.jks -exportcert -alias CARoot -rfc -file ca_cert.pem -storepass $SSL_KEYSTORE_PASSWORD`

Now you are prepared with the four values needed to configure SSL connection in TriXX.

=== Running TriXX ===
The TriXX application is provided as Docker-Image by:
[source]
docker pull git.osp-dd.de:5005/main/trixx

==== Start Docker container ====
You can run the this image like:
[source]
docker run -p 8080:8080 \
  --stop-timeout=120 \
  -e TRIXX_RUN_CONFIG=/etc/trixx_run.yml \
  -v /my_local_dir/trixx_run.yml:/etc/trixx_run.yml \
  git.osp-dd.de:5005/main/trixx

The web-GUI would be available by http://localhost:8080 in this case.
It is recommended to place an own reverse proxy nearby for SSL encryption.

==== Stop Docker container ====
To stop the Docker container you should provide a timeout (at "docker run" or with "docker stop") that allows TriXX to gracefully shutdown all worker threads before Docker terminates hard with "kill -9".

 docker stop -t 120 trixx

==== Uniqueness of TriXX instances ====
Depending on the database type you may run multiple TriXX instances at one database or not.

.Multiple instances allowed for TriXX
[cols="~,~,~"]
|===
|DB type|Multiple instances with same configuration (same TriXX schema)|Multiple instances with different configuration (different TriXX schemas, different Kafka targets)

|SQLite
|Not allowed: No synchronization between multiple instances exist
|Not allowed: No config-specific trigger names are used
|ORACLE
|Possible: Messages to transfer to Kafka are selected with FOR UPDATE.
|Possible: Trigger names contain numeric hash value of TriXX' owner schema. +
Therefore multiple triggers from several independent TriXX configurations at one table are possible.
|===


=== Configuration of capturing with TriXX-GUI ===
TODO: Describe GUI workflow

==== Managing users, schemas and rights ====
Menu "Users" shows the already created named users for TriXX. Initially there is always a predefined user 'admin'. +
Users are identified by E-Mail.
For authentification at logon one DB-User is associated to each TriXX user, the password of this DB-user is used for logon.

The TriXX-user is authorized for certain schemas for which tables can be tagged for event capturing.
This schemas can be picked from the list of schemas where the user has select grants at at least one table of this schema.

==== Configuring events to capture at column level ====
This dialog shows:

* schemas for which the user has the right to configure (via TriXX user configuration)
* already configured tables of a schema (limited to tables where the user has SELECT grants for)
* columns of a configured table with marks for Insert/Update/Delete-trigger

Possible onfiguration actions are:

* add tables to configuration for a schema (only possible for tables where the user has SELECT grant for)
* modify topic name per table
* modify triggering of change events per column

NOTE: The configuration in this screen is not user-specific. Each table/column configuration exists only once and can be manipulated by several permitted users.

==== Generation of database triggers ====

=== Troubleshooting ===
==== Health check ====
There is a healthcheck service available at:

 http://<TriXX-URL>/health_check

Status code 200 (ok) is returned if the configured number of worker threads exists and is functional. +
The http-response contains a JSON-object with detailled informations.

==== Common error traps ====
===== Kafka =====
.possible problems accessing or using Kafka
[cols="~,~,~"]
|===
|Error|Description|Solution
|Log-Output: +
Disconnecting broker 0 +
Closing socket to kafka-itdr-dev:9092 +
Module: Kafka::UnknownError: Unknown error with code 53
|Error 53 means: TRANSACTIONAL_ID_AUTHORIZATION_FAILED +
The transactional id used by TriXX is not authorized to produce messages
|Explicite authorization of transactional id is required, optional as wildcard: +
kafka-acls --bootstrap-server localhost:9092 --command-config adminclient-configs.conf
--add --transactional-id * --allow-principal User:* --operation write
|===



===== Oracle-DB =====
* If TNS alias is used for TRIXX_DB_URL but no tnsnames.ora available at TNS_ADMIN then the JDBC driver treats the TNS alias as host:port:sid with several possible error messages (host does not exist etc.)

== Technical implementation ==
=== Structure of Kafka messages ===
TriXX creates Kafka messages with JSON-formatted content. +
Depending on table configuration Kafka messages may contain an additional key value which drives the assignment of messages to partitions (messages with same key are stored in the same partition).

.Value conversion from database column to JSON value
[cols="~,~,~"]
|===
|JSON representation|Example|Oracle data types

|Number|45.23|BINARY_DOUBLE, BINARY_FLOAT, FLOAT, NUMBER
|String|"Value"|CHAR, CLOB, NCHAR, NCLOB, NVARCHAR2, LONG, ROWID, UROWID, VARCHAR2
|String|"2020-02-21T12:07:43"|DATE
|String|"2020-02-21T12:07:43,396153000"|TIMESTAMP
|String|"2020-02-21T12:07:43,396142000+00:00"|TIMESTAMP WITH TIME ZONE
|String|"90FF"|RAW
|===


.Field names used in Kafka message
[cols="~,~"]
|===
|Fieldname|Explanation

|id|consecutive unique message ID, describes the order of message creation at database trigger level
|schema|schema name of database table
|tablename|name of database table
|operation|kind of triggering database operation
|dbuser|database user who run the triggering operation
|timestamp|detailled timestamp of triggering event
|old|values of observed columns before triggering change event
|new|values of observed columns after triggering event
|===

==== Example of INSERT message ====

[source, json]
{
  "id": 23423274179,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "INSERT",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "new": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": NULL
  }
}

==== Example of UPDATE message ====

[source, json]
{
  "id": 234232741379,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "UPDATE",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "old": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": NULL
  },
  "new": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAACAAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": NULL
  }
}

==== Example of DELETE message ====
[source, json]
{
  "id": 2342327412279,
  "schema": "EINKAUF",
  "tablename": "HUGO",
  "operation": "DELETE",
  "dbuser": "MEYER",
  "timestamp": "2020-02-21T12:07:43,396142+00:00",
  "old": {
    "ID": 1,
    "NAME": "Record1",
    "CHAR_NAME": "Y",
    "DATE_VAL": "2020-02-21T12:07:43",
    "TS_VAL": "2020-02-21T12:07:43,396153000",
    "RAW_VAL": "FFFF",
    "TSTZ_VAL": "2020-02-21T12:07:43,396142000+00:00",
    "ROWID_VAL": "AAAUQ6AAMAAAAJlAAC",
    "NULL_VAL": NULL
  }
}


=== Module overview ===
image:trixx_module_overview.svg.drawio[format=svg,opts=inline]

==== Activities at application startup ====

The following things are executed at startup of application / docker container if necessary:

* The needed data structures in TriXX-schema (TRIXX_DB_USER) are created or updated
* The initial TriXX-user "admin" is created for GUI with link to the TRIXX_DB_USER
** For initial GUI logon with user "admin" the password is the DB-passwort of Trixx DB-user (TRIXX_DB_PASSWORD)
** The GUI user "admin" acts as supervisor with the authorization to administrate further user accounts

==== Generation of database triggers ====
==== Transfer of triggered events to Kafka ====
An consecutive ID is used to define the order of message creation at trigger level. +
This ID allows the reconstruction of the original order of messages in Kafka even if using topics with multiple partitions.

NOTE: For Oracle-DB: If using RAC this ID represents the original order only per RAC-instance because a cached sequence is used for value generation.

Message creation in Kafka is done by TriXX with multiple concurrent threads. +
It is not guaranteed that messages are created in Kafka in the order of the ID.

==== Houskeeping ====

==== Health check ====

=== API endpoints ===
Most of the API endpoints are useful only when called from GUI, but several of this API endpoints may also be useful for calling from outside TriXX. +
API Responses are JSON objects.

.API endpoints for additional usage from outside TriXX application
[cols="~,~,~,~,~"]
|===
|Verb|URL|Parameter|Response|Description

|GET|/health_check|no|JSON object with several application status info|ask health status (200=ok) and get some condensed status information
|GET|/health_check/log_file|no|current log file of application|Download log file of TriXX application. +
Requires valid user JWT in request header.
|POST|/login/do_logon|email, password|token|Validate user authentication, get JWT token for authentication/authorization of following requests
|POST|/server_control/set_log_level|log_level (DEBUG, INFO, WARN, ERROR, FATAL)|no|Set log level of server instance, requires valid admin JWT in request header
|POST|/server_control/set_worker_threads_count|worker_threads_count (0..200)|no|Set number of active worker threads, requires valid admin JWT in request header
|POST|/server_control/terminate|no|no|Terminate the current TriXX process by sending SIGTERM, requires valid admin JWT in request header
|===

==== Using API endpoints from command line ====
You can use curl or wget to call API funktions with valid autorization by email and password. +
Example is for setting log level to DEBUG, adjust hostname, port, email and password to your needs. +
Needed tools are curl, jq, sed.

[source]
curl -X POST -H "Authorization: \
`curl -d "email=admin&password=trixx" http://localhost:8080/login/do_logon | \
jq .token | sed -e 's/^"//' -e 's/"$//'`" \
-d "log_level=ERROR" \
http://localhost:8080/server_control/set_log_level

=== Entity relationship model ===
image:trixx_er_model.svg.drawio[format=svg,opts=inline]

=== Security considerations ===
==== Lock user account after multiple failed logons ====
User account is locked after 3 subsequent failed logon tries. +
Unlocking a locked account is possible via GUI for admin users.

==== Suppress frequent access ====
* Email/password check at /login/do_logon is delayed for up to 5 seconds if subsequent logon requests occur within 5 seconds
* Subsequent calls to /health_check are rejected within the same second

==== Isolation between production schemas at database and TriXX ====
The TriXX application requires an own schema on database for TriXX. This schema must not contain any foreign structures.
All database changes made by TriXX are isolated to this schema.
The owner of the TriXX-schema requires only a minimum set of rights on foreign objects, especially no right to read the full table content.

==== Restricted definition of triggers ====
There might be a security gap if users may define trigger on tables where they don't have read rights.
This way they could possibly read hidden table content via Kafka. +
Therefore only tables are accessible for trigger definition at TriXX-GUI where the connected user has at least read rights.
